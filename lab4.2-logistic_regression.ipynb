{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression based Sentiment Analysis\n",
    "In this notebook, we will explore the use of logistic regression (LR) for sentiment analysis. In particular, we will not only use existing LR implementation to perform the clasification but, more importantly, also look into how to select features and study how features influence the model performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain and split data \n",
    "from nltk.corpus import movie_reviews\n",
    "import random\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "random.shuffle(documents)\n",
    "\n",
    "train_data = documents[:1200]\n",
    "dev_data = documents[1200:1600]\n",
    "test_data = documents[1600:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use tf-idf vectors to represent text, use logistic regression as classifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(use_idf=False)\n",
    "train_text = [' '.join(tokens) for (tokens,label) in train_data]\n",
    "train_labels = [label for (tokens,label) in train_data]\n",
    "tfidf_vectorizer.fit(train_text)\n",
    "train_vecs = tfidf_vectorizer.transform(train_text)\n",
    "clf = LogisticRegression().fit(train_vecs, train_labels)\n",
    "\n",
    "dev_text = [' '.join(tokens) for (tokens,label) in dev_data]\n",
    "dev_vecs = tfidf_vectorizer.transform(dev_text)\n",
    "dev_pred_labels = clf.predict(dev_vecs)\n",
    "dev_true_labels = [label for (tokens,label) in dev_data]\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "print('acc', accuracy_score(dev_true_labels, dev_pred_labels))\n",
    "print('precision, recall, f1, support', precision_recall_fscore_support(dev_true_labels, dev_pred_labels, average=None, labels=['pos', 'neg']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selection Features\n",
    "The code above uses standard tf-idf vectorizer to represent texts. Function *TfidfVectorizer* allows you to customize the features you want to used in the vectors. The function manual can be found [here](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html). Notably, you may consider play with the following options and find which combination yields the best performance on the dev set.\n",
    "* **lowercase**: bool (default=True). If True, the vectorizer will convert all characters to lowercase before tokenizing.\n",
    "* **stop_words**: {‘english’}, list, or None (default=None). Whether to remove stopwords. You are allowed to specify the stopwords list\n",
    "* **ngram_range**: tuple (min_n, max_n), default=(1, 1). This option allows you to specify which n-grams will be used to build the vocabulary. For example, if you let ngram_range=(1,2), your vocabulary will include all uni-grams and bi-grams in your training set.\n",
    "* **max_df**: float in range \\[0.0, 1.0\\] or int (default=1.0). You can exclude types that appear in too many documents (i.e. types that have too high document-frequency values). High-df words are often common words and hence are less informative.\n",
    "* **min_df**: float in range \\[0.0, 1.0\\] or int (default=1). Contrary to the last option, this option allows you to exclude low-df words from your vocabulary. Low-df words are usually not representative enough; removing them can help to reduce the feature number and hence avoid sparse vectors.\n",
    "* **max_features**: int or None (default=None). If not None, build a vocabulary that only consider the top max_features ordered by term frequency across the corpus. Again, this option allows you to reduce the feature number.\n",
    "* **vocabulary**: Mapping or iterable, optional (default=None). This option allows you to specify the vocabulary used to build the vectors. Particulary useful when vectorizing text in the dev and test set, because you should use the vocabulary built at training time.\n",
    "* **binary**: bool (default=False). If True, all non-zero term counts are set to 1.\n",
    "* **use_idf**: bool (default=True). Whether idf is used.\n",
    "* **smooth_idf**: bool (default=True). Smooth idf weights by adding one to document frequencies, as if an extra document was seen containing every term in the collection exactly once. Prevents zero divisions.\n",
    "* **sublinear_tf**: bool (default=False). Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a function that allows you to try different options combinations, and returns the \n",
    "# performance of each combination and the trained model. \n",
    "\n",
    "def get_model_performance(train_text, train_labels, dev_text, dev_labels, options):\n",
    "    tfidf_vectorizer = TfidfVectorizer() #apply options to the vectorizer\n",
    "    tfidf_vectorizer.fit(train_text)\n",
    "    train_vecs = tfidf_vectorizer.transform(train_text)\n",
    "    clf = LogisticRegression().fit(train_vecs, train_labels)\n",
    "    \n",
    "    dev_vecs = tfidf_vectorizer.transform(dev_text)\n",
    "    dev_pred_labels = clf.predict(dev_vecs)\n",
    "\n",
    "    return accuracy_score(dev_labels, dev_pred_labels), clf, tfidf_vectorizer\n",
    "\n",
    "\n",
    "train_text = [' '.join(tokens) for (tokens,label) in train_data]\n",
    "train_labels = [label for (tokens,label) in train_data]\n",
    "dev_text = [' '.join(tokens) for (tokens,label) in dev_data]\n",
    "dev_labels = [label for (tokens,label) in dev_data]\n",
    "\n",
    "option_combos = [None] # try different combinations of options\n",
    "best_acc = -1\n",
    "best_clf = None\n",
    "best_vectorizer = None\n",
    "for option in option_combos:\n",
    "    acc, clf, vectorizer = get_model_performance(train_text, train_labels, dev_text, dev_labels, option)\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_clf = clf\n",
    "        best_vectorizer = vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the best obtained model and vectorizer to predict test data\n",
    "test_text = [' '.join(tokens) for (tokens,label) in test_data]\n",
    "test_vecs = best_vectorizer.transform(test_text)\n",
    "test_pred_labels = best_clf.predict(test_vecs)\n",
    "test_true_labels = [label for (tokens, label) in test_data]\n",
    "\n",
    "print('acc', accuracy_score(test_true_labels, test_pred_labels))\n",
    "print('precision, recall, f1, support', precision_recall_fscore_support(test_true_labels, test_pred_labels, average=None, labels=['pos', 'neg']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_nlp",
   "language": "python",
   "name": "venv_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
