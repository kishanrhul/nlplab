{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip-gram word embedding \n",
    "In this notebook, we train a word embedding model using the skip-gram technique. In particular, we use both the *dot product* trick and the *negative sampling* trick to speed up learning. Given that said, training a word embedding is still highly expensive: you should expect a few hours of training if you want to train a good word embedding (more time would probably be required if you only use CPU!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n"
     ]
    }
   ],
   "source": [
    "# train our model using brown corpus\n",
    "from nltk.corpus import brown\n",
    "print(brown.categories())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sometimes', 'he', 'woke', 'up', 'in', 'the', 'middle', 'of', 'the', 'night', 'thinking', 'of', 'Ann', ',', 'and', 'then', 'could', 'not', 'get', 'back', 'to', 'sleep', '.']\n",
      "57340\n"
     ]
    }
   ],
   "source": [
    "sents = []\n",
    "for genre in brown.categories():\n",
    "    sents += brown.sents(categories=genre)\n",
    "print(sents[5])\n",
    "print(len(sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "008ddfdef8794fde9de6a41367e33494",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/57340 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# lower-case and stem all tokens\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "all_tokens = []\n",
    "train_sents = []\n",
    "\n",
    "for ss in tqdm(sents):\n",
    "    train_sents.append([stemmer.stem(token.lower()) for token in ss])\n",
    "    all_tokens += train_sents[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 69971),\n",
       " (',', 58334),\n",
       " ('.', 49346),\n",
       " ('of', 36413),\n",
       " ('and', 28853),\n",
       " ('to', 26158),\n",
       " ('a', 23195),\n",
       " ('in', 21337),\n",
       " ('it', 10618),\n",
       " ('that', 10594)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "fd = FreqDist(all_tokens)\n",
    "fd.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size 34543\n",
      "['poshest', '$0.9', 'hungarian-born', 'invigor', 'charg', 'center-punch', 'writ', 'inflect', 'springtim', '1800']\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary\n",
    "\n",
    "vocab = list(set(all_tokens))\n",
    "print('vocab size', len(vocab))\n",
    "print(vocab[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build dictionaries to help us mapping between words and their indices\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "word_to_idx = {word:idx for idx, word in enumerate(vocab)}\n",
    "idx_to_word = {idx:word for idx, word in enumerate(vocab)}\n",
    "embd_dim = 100 # dimension of the wanted word vectors; a strong word vector would require 100 or more dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.7216,  1.3227,  0.5370, -0.1401,  2.2933])\n"
     ]
    }
   ],
   "source": [
    "# example code for computing pairwise dot product\n",
    "# we will use this trick below in EmbeddingLearner\n",
    "max1 = torch.randn(5,10)\n",
    "max2 = torch.randn(5,10)\n",
    "dot_product = torch.bmm(max1.view(5,1,10),max2.view(5,10,1)).squeeze()\n",
    "print(dot_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the neural network for learning embeddings\n",
    "class EmbedingLearner(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab, embd_dim, device):\n",
    "        super(EmbedingLearner, self).__init__()\n",
    "        self.u_vecs = nn.Embedding(len(vocab), embd_dim)\n",
    "        self.v_vecs = nn.Embedding(len(vocab), embd_dim)\n",
    "        self.embd_dim = embd_dim\n",
    "        self.device = device\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, word_pairs_idx):\n",
    "        pair_num = len(word_pairs_idx)\n",
    "        center_lookup = torch.tensor([pair[0] for pair in word_pairs_idx], dtype=torch.long).to(self.device)\n",
    "        context_lookup = torch.tensor([pair[1] for pair in word_pairs_idx], dtype=torch.long).to(self.device)\n",
    "        center_vecs = self.u_vecs(center_lookup)\n",
    "        context_vecs = self.v_vecs(context_lookup)\n",
    "        sim_scores = torch.bmm(center_vecs.view(pair_num, 1, self.embd_dim), context_vecs.view(pair_num, self.embd_dim, 1)).squeeze()\n",
    "        return sim_scores\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for constructing mini-batches\n",
    "import random\n",
    "def get_mini_batch(word_to_idx, tokens, center_idx, win_size):\n",
    "    vocab_num = len(word_to_idx)\n",
    "    center_word = tokens[center_idx]\n",
    "    center_word_idx = word_to_idx[center_word]\n",
    "    word_pairs = []\n",
    "    # first we build positive examples, i.e. center words with real context words\n",
    "    for i in range(center_idx-win_size, center_idx+win_size+1):\n",
    "        if i < 0 or i >= len(tokens): continue\n",
    "        if i == center_idx: continue\n",
    "        context_word = tokens[i]\n",
    "        word_pairs.append( (center_word_idx, word_to_idx[context_word], 1) )\n",
    "    # then we use negative sampling to find some non-context words\n",
    "    pos_examples_num = len(word_pairs)\n",
    "    context_word_idx = set([tup[1] for tup in word_pairs])\n",
    "    while len(word_pairs) < 2*pos_examples_num:\n",
    "        neg_word_idx = random.randint(0,vocab_num-1)\n",
    "        while neg_word_idx in context_word_idx:\n",
    "            neg_word_idx = random.randint(0,vocab_num-1)\n",
    "        word_pairs.append( (center_word_idx, neg_word_idx, -1) )\n",
    "    return word_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4853, 12795, 1),\n",
       " (4853, 12360, 1),\n",
       " (4853, 9497, 1),\n",
       " (4853, 4445, 1),\n",
       " (4853, 31447, -1),\n",
       " (4853, 19437, -1),\n",
       " (4853, 4461, -1),\n",
       " (4853, 28007, -1)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the mini_batch constructor\n",
    "get_mini_batch(word_to_idx, train_sents[10], center_idx=3, win_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9fd9e625c5b4865ba687b305b0a6f88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34543 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cim/staff/uhac002/PycharmProjects/ScratchPad/venv_nlp/lib/python3.7/site-packages/ipykernel_launcher.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/home/cim/staff/uhac002/PycharmProjects/ScratchPad/venv_nlp/lib/python3.7/site-packages/ipykernel_launcher.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('read', 0.9999999), ('cabdriv', 0.39111134), ('honest-to-betsi', 0.3885032), (\"peabody'\", 0.3740201), ('admonit', 0.36070418), ('kahler-craft', 0.35043496), ('pickoff', 0.34816372), (\"dartmouth'\", 0.34729564), ('barton', 0.34363657), ('folk-danc', 0.34258938)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# initialize the embedding learner\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "embd_learner = EmbedingLearner(vocab, embd_dim, device)\n",
    "\n",
    "def get_word_vec(embd_learner, word):\n",
    "    if word not in vocab: return None\n",
    "    lookup = torch.tensor(word_to_idx[word], dtype=torch.long).to(device)\n",
    "    u_vec = embd_learner.u_vecs(torch.tensor(lookup)).cpu().detach().numpy()\n",
    "    v_vec = embd_learner.v_vecs(torch.tensor(lookup)).cpu().detach().numpy()\n",
    "    return np.mean([v_vec, u_vec], axis=0)\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def get_most_similar(word, word_vecs):\n",
    "    similar_list = []\n",
    "    word_vec = word_vecs[word]\n",
    "    all_vecs = np.array(list(word_vecs.values())).reshape(-1,embd_dim)\n",
    "    cos_sims = list(cosine_similarity(word_vec.reshape(1,-1), all_vecs)[0])\n",
    "    sorted_cos_sims = sorted(cos_sims,reverse=True)\n",
    "    for v in sorted_cos_sims:\n",
    "        word_id = cos_sims.index(v)\n",
    "        similar_list.append((idx_to_word[word_id], v))\n",
    "    return similar_list\n",
    "    \n",
    "# get word vectors (before training)\n",
    "word_vecs = {}\n",
    "for word in tqdm(vocab):\n",
    "    vec = get_word_vec(embd_learner,word)\n",
    "    if vec is not None:\n",
    "        word_vecs[word] = vec\n",
    "\n",
    "# print the most similar words for a given word, to see how the word embeddings work\n",
    "# note that we have not trained our word embeddings here\n",
    "# hence the most similar words should be rather random\n",
    "print(get_most_similar('read', word_vecs)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embd_learner = EmbedingLearner(vocab, embd_dim, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9efcaaef116d478ba8a0466f9c7f7a6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35b25bba1dcb4f739c9e8f17d0fdaa6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sent:   0%|          | 0/57340 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg loss until step 500: 0.5654630661010742\n",
      "avg loss until step 1000: -0.4153224527835846\n",
      "avg loss until step 1500: -0.2386307567358017\n",
      "avg loss until step 2000: -0.6134337782859802\n",
      "avg loss until step 2500: -0.979987382888794\n",
      "avg loss until step 3000: -1.3266180753707886\n",
      "avg loss until step 3500: -1.5214871168136597\n",
      "avg loss until step 4000: -1.6783417463302612\n",
      "avg loss until step 4500: -2.046870470046997\n",
      "avg loss until step 5000: -2.3686416149139404\n",
      "avg loss until step 5500: -2.6272451877593994\n",
      "avg loss until step 6000: -3.0577385425567627\n",
      "avg loss until step 6500: -3.548492193222046\n",
      "avg loss until step 7000: -4.0123138427734375\n",
      "avg loss until step 7500: -4.525312423706055\n",
      "avg loss until step 8000: -4.9944539070129395\n",
      "avg loss until step 8500: -5.722128391265869\n",
      "avg loss until step 9000: -6.3657097816467285\n",
      "avg loss until step 9500: -6.937943458557129\n",
      "avg loss until step 10000: -7.338605880737305\n",
      "avg loss until step 10500: -7.801496982574463\n",
      "avg loss until step 11000: -8.224465370178223\n",
      "avg loss until step 11500: -8.738534927368164\n",
      "avg loss until step 12000: -9.266312599182129\n",
      "avg loss until step 12500: -9.75535774230957\n",
      "avg loss until step 13000: -10.255560874938965\n",
      "avg loss until step 13500: -10.721735000610352\n",
      "avg loss until step 14000: -11.11550521850586\n",
      "avg loss until step 14500: -11.747907638549805\n",
      "avg loss until step 15000: -12.25414752960205\n",
      "avg loss until step 15500: -13.074151992797852\n",
      "avg loss until step 16000: -13.853002548217773\n",
      "avg loss until step 16500: -14.652734756469727\n",
      "avg loss until step 17000: -15.378294944763184\n",
      "avg loss until step 17500: -15.830424308776855\n",
      "avg loss until step 18000: -16.320262908935547\n",
      "avg loss until step 18500: -16.906888961791992\n",
      "avg loss until step 19000: -17.489229202270508\n",
      "avg loss until step 19500: -18.223209381103516\n",
      "avg loss until step 20000: -18.663711547851562\n",
      "avg loss until step 20500: -19.328725814819336\n",
      "avg loss until step 21000: -20.09105682373047\n",
      "avg loss until step 21500: -21.011192321777344\n",
      "avg loss until step 22000: -21.725452423095703\n",
      "avg loss until step 22500: -22.557682037353516\n",
      "avg loss until step 23000: -23.37712287902832\n",
      "avg loss until step 23500: -24.388965606689453\n",
      "avg loss until step 24000: -25.33890724182129\n",
      "avg loss until step 24500: -26.243999481201172\n",
      "avg loss until step 25000: -27.465085983276367\n",
      "avg loss until step 25500: -28.534706115722656\n",
      "avg loss until step 26000: -29.672849655151367\n",
      "avg loss until step 26500: -31.040184020996094\n",
      "avg loss until step 27000: -32.26077651977539\n",
      "avg loss until step 27500: -33.083248138427734\n",
      "avg loss until step 28000: -34.176971435546875\n",
      "avg loss until step 28500: -35.365909576416016\n",
      "avg loss until step 29000: -36.19313049316406\n",
      "avg loss until step 29500: -37.43305206298828\n",
      "avg loss until step 30000: -38.620059967041016\n",
      "avg loss until step 30500: -39.68741226196289\n",
      "avg loss until step 31000: -40.8776741027832\n",
      "avg loss until step 31500: -42.350894927978516\n",
      "avg loss until step 32000: -44.22809600830078\n",
      "avg loss until step 32500: -45.69398880004883\n",
      "avg loss until step 33000: -46.5471305847168\n",
      "avg loss until step 33500: -47.476806640625\n",
      "avg loss until step 34000: -48.35348892211914\n",
      "avg loss until step 34500: -49.26553726196289\n",
      "avg loss until step 35000: -50.16775894165039\n",
      "avg loss until step 35500: -51.622596740722656\n",
      "avg loss until step 36000: -52.927547454833984\n",
      "avg loss until step 36500: -54.304935455322266\n",
      "avg loss until step 37000: -55.755619049072266\n",
      "avg loss until step 37500: -57.0731086730957\n",
      "avg loss until step 38000: -58.76002883911133\n",
      "avg loss until step 38500: -60.492156982421875\n",
      "avg loss until step 39000: -62.5709114074707\n",
      "avg loss until step 39500: -64.90518188476562\n",
      "avg loss until step 40000: -67.16159057617188\n",
      "avg loss until step 40500: -68.81410217285156\n",
      "avg loss until step 41000: -71.03973388671875\n",
      "avg loss until step 41500: -73.2826919555664\n",
      "avg loss until step 42000: -75.12248229980469\n",
      "avg loss until step 42500: -76.54798126220703\n",
      "avg loss until step 43000: -78.51605224609375\n",
      "avg loss until step 43500: -79.6729507446289\n",
      "avg loss until step 44000: -81.00138092041016\n",
      "avg loss until step 44500: -82.86460876464844\n",
      "avg loss until step 45000: -84.56460571289062\n",
      "avg loss in epoch 0: -85.579163\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d80ced1d2824ef8b708d9da98556987",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sent:   0%|          | 0/57340 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg loss until step 500: -277.73687744140625\n",
      "avg loss until step 1000: -279.0705871582031\n",
      "avg loss until step 1500: -277.1864013671875\n",
      "avg loss until step 2000: -275.29010009765625\n",
      "avg loss until step 2500: -284.4161376953125\n",
      "avg loss until step 3000: -289.00189208984375\n",
      "avg loss until step 3500: -299.31292724609375\n",
      "avg loss until step 4000: -314.55706787109375\n",
      "avg loss until step 4500: -326.62945556640625\n",
      "avg loss until step 5000: -336.1578063964844\n",
      "avg loss until step 5500: -344.5446472167969\n",
      "avg loss until step 6000: -351.8552551269531\n",
      "avg loss until step 6500: -357.798828125\n",
      "avg loss until step 7000: -365.8656921386719\n",
      "avg loss until step 7500: -371.743896484375\n",
      "avg loss until step 8000: -380.8100280761719\n",
      "avg loss until step 8500: -392.4014892578125\n",
      "avg loss until step 9000: -401.3846435546875\n",
      "avg loss until step 9500: -409.787353515625\n",
      "avg loss until step 10000: -414.1379699707031\n",
      "avg loss until step 10500: -417.41571044921875\n",
      "avg loss until step 11000: -421.9937744140625\n",
      "avg loss until step 11500: -427.7929382324219\n",
      "avg loss until step 12000: -433.4723815917969\n",
      "avg loss until step 12500: -438.4537353515625\n",
      "avg loss until step 13000: -445.1458435058594\n",
      "avg loss until step 13500: -450.197998046875\n",
      "avg loss until step 14000: -453.6094055175781\n",
      "avg loss until step 14500: -460.602294921875\n",
      "avg loss until step 15000: -466.5389404296875\n",
      "avg loss until step 15500: -477.3826599121094\n",
      "avg loss until step 16000: -487.4276428222656\n",
      "avg loss until step 16500: -498.24700927734375\n",
      "avg loss until step 17000: -506.4311828613281\n",
      "avg loss until step 17500: -510.60662841796875\n",
      "avg loss until step 18000: -515.1444702148438\n",
      "avg loss until step 18500: -520.8901977539062\n",
      "avg loss until step 19000: -526.5784912109375\n",
      "avg loss until step 19500: -534.7930908203125\n",
      "avg loss until step 20000: -537.9428100585938\n",
      "avg loss until step 20500: -543.3818359375\n",
      "avg loss until step 21000: -550.7560424804688\n",
      "avg loss until step 21500: -560.4588623046875\n",
      "avg loss until step 22000: -567.7008056640625\n",
      "avg loss until step 22500: -575.7252197265625\n",
      "avg loss until step 23000: -583.8661499023438\n",
      "avg loss until step 23500: -595.3104858398438\n",
      "avg loss until step 24000: -604.6712646484375\n",
      "avg loss until step 24500: -613.069580078125\n",
      "avg loss until step 25000: -624.8273315429688\n",
      "avg loss until step 25500: -634.9829711914062\n",
      "avg loss until step 26000: -645.8770751953125\n",
      "avg loss until step 26500: -659.111572265625\n",
      "avg loss until step 27000: -670.1458129882812\n",
      "avg loss until step 27500: -676.6640014648438\n",
      "avg loss until step 28000: -685.8384399414062\n",
      "avg loss until step 28500: -695.993408203125\n",
      "avg loss until step 29000: -702.5037841796875\n",
      "avg loss until step 29500: -712.9676513671875\n",
      "avg loss until step 30000: -722.7239379882812\n",
      "avg loss until step 30500: -731.00439453125\n",
      "avg loss until step 31000: -740.58056640625\n",
      "avg loss until step 31500: -752.3394775390625\n",
      "avg loss until step 32000: -767.6506958007812\n",
      "avg loss until step 32500: -778.8084716796875\n",
      "avg loss until step 33000: -784.7500610351562\n",
      "avg loss until step 33500: -791.2965698242188\n",
      "avg loss until step 34000: -797.314453125\n",
      "avg loss until step 34500: -803.48779296875\n",
      "avg loss until step 35000: -809.5006103515625\n",
      "avg loss until step 35500: -819.497314453125\n",
      "avg loss until step 36000: -828.061767578125\n",
      "avg loss until step 36500: -836.86328125\n",
      "avg loss until step 37000: -846.260986328125\n",
      "avg loss until step 37500: -854.439697265625\n",
      "avg loss until step 38000: -865.57470703125\n",
      "avg loss until step 38500: -876.7525634765625\n",
      "avg loss until step 39000: -890.8809814453125\n",
      "avg loss until step 39500: -906.7919921875\n",
      "avg loss until step 40000: -921.4169921875\n",
      "avg loss until step 40500: -931.7178955078125\n",
      "avg loss until step 41000: -945.7349853515625\n",
      "avg loss until step 41500: -959.5244750976562\n",
      "avg loss until step 42000: -970.2830200195312\n",
      "avg loss until step 42500: -978.3583374023438\n",
      "avg loss until step 43000: -990.0645141601562\n",
      "avg loss until step 43500: -996.0863647460938\n",
      "avg loss until step 44000: -1003.0767822265625\n",
      "avg loss until step 44500: -1013.9616088867188\n",
      "avg loss until step 45000: -1023.349853515625\n",
      "avg loss in epoch 1: -1028.765991\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b7045da933c4c259d2fa20785249494",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sent:   0%|          | 0/57340 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg loss until step 500: -2067.375244140625\n",
      "avg loss until step 1000: -2066.0966796875\n",
      "avg loss until step 1500: -2043.3858642578125\n",
      "avg loss until step 2000: -2020.3819580078125\n",
      "avg loss until step 2500: -2070.76123046875\n",
      "avg loss until step 3000: -2091.785888671875\n",
      "avg loss until step 3500: -2151.398193359375\n",
      "avg loss until step 4000: -2243.364501953125\n",
      "avg loss until step 4500: -2312.21826171875\n",
      "avg loss until step 5000: -2362.970703125\n",
      "avg loss until step 5500: -2406.37255859375\n",
      "avg loss until step 6000: -2439.256591796875\n",
      "avg loss until step 6500: -2461.867431640625\n",
      "avg loss until step 7000: -2498.333251953125\n",
      "avg loss until step 7500: -2518.751220703125\n",
      "avg loss until step 8000: -2560.49267578125\n",
      "avg loss until step 8500: -2611.74755859375\n",
      "avg loss until step 9000: -2646.80615234375\n",
      "avg loss until step 9500: -2680.222412109375\n",
      "avg loss until step 10000: -2691.02685546875\n",
      "avg loss until step 10500: -2695.86669921875\n",
      "avg loss until step 11000: -2708.39306640625\n",
      "avg loss until step 11500: -2726.47412109375\n",
      "avg loss until step 12000: -2743.125732421875\n",
      "avg loss until step 12500: -2758.797119140625\n",
      "avg loss until step 13000: -2782.4697265625\n",
      "avg loss until step 13500: -2797.406982421875\n",
      "avg loss until step 14000: -2804.4296875\n",
      "avg loss until step 14500: -2827.140380859375\n",
      "avg loss until step 15000: -2844.25\n",
      "avg loss until step 15500: -2883.1123046875\n",
      "avg loss until step 16000: -2918.021484375\n",
      "avg loss until step 16500: -2954.945068359375\n",
      "avg loss until step 17000: -2979.84521484375\n",
      "avg loss until step 17500: -2987.868896484375\n",
      "avg loss until step 18000: -2998.10498046875\n",
      "avg loss until step 18500: -3013.44677734375\n",
      "avg loss until step 19000: -3028.537353515625\n",
      "avg loss until step 19500: -3054.128173828125\n",
      "avg loss until step 20000: -3056.800537109375\n",
      "avg loss until step 20500: -3068.597412109375\n",
      "avg loss until step 21000: -3089.47802734375\n",
      "avg loss until step 21500: -3120.067138671875\n",
      "avg loss until step 22000: -3139.962890625\n",
      "avg loss until step 22500: -3161.897216796875\n",
      "avg loss until step 23000: -3184.63330078125\n",
      "avg loss until step 23500: -3221.4140625\n",
      "avg loss until step 24000: -3248.345703125\n",
      "avg loss until step 24500: -3271.384765625\n",
      "avg loss until step 25000: -3306.201171875\n",
      "avg loss until step 25500: -3335.198974609375\n",
      "avg loss until step 26000: -3366.917236328125\n",
      "avg loss until step 26500: -3406.971923828125\n",
      "avg loss until step 27000: -3438.0439453125\n",
      "avg loss until step 27500: -3453.310546875\n",
      "avg loss until step 28000: -3477.111083984375\n",
      "avg loss until step 28500: -3504.84619140625\n",
      "avg loss until step 29000: -3519.843994140625\n",
      "avg loss until step 29500: -3548.33935546875\n",
      "avg loss until step 30000: -3573.979248046875\n",
      "avg loss until step 30500: -3594.943603515625\n",
      "avg loss until step 31000: -3619.491455078125\n",
      "avg loss until step 31500: -3651.989501953125\n",
      "avg loss until step 32000: -3695.403076171875\n",
      "avg loss until step 32500: -3724.843994140625\n",
      "avg loss until step 33000: -3738.053955078125\n",
      "avg loss until step 33500: -3752.979736328125\n",
      "avg loss until step 34000: -3766.432373046875\n",
      "avg loss until step 34500: -3779.65673828125\n",
      "avg loss until step 35000: -3792.67578125\n",
      "avg loss until step 35500: -3815.828857421875\n",
      "avg loss until step 36000: -3834.51806640625\n",
      "avg loss until step 36500: -3854.183349609375\n",
      "avg loss until step 37000: -3875.1689453125\n",
      "avg loss until step 37500: -3892.1708984375\n",
      "avg loss until step 38000: -3918.92822265625\n",
      "avg loss until step 38500: -3945.11181640625\n",
      "avg loss until step 39000: -3981.24267578125\n",
      "avg loss until step 39500: -4022.482177734375\n",
      "avg loss until step 40000: -4059.3369140625\n",
      "avg loss until step 40500: -4083.456787109375\n",
      "avg loss until step 41000: -4117.80029296875\n",
      "avg loss until step 41500: -4151.13037109375\n",
      "avg loss until step 42000: -4175.1416015625\n",
      "avg loss until step 42500: -4192.46044921875\n",
      "avg loss until step 43000: -4220.07861328125\n",
      "avg loss until step 43500: -4231.2783203125\n",
      "avg loss until step 44000: -4245.16015625\n",
      "avg loss until step 44500: -4271.0634765625\n",
      "avg loss until step 45000: -4291.8203125\n",
      "avg loss in epoch 2: -4303.370605\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "lr = 1e-5\n",
    "window_size = 10\n",
    "\n",
    "# init optimizer\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "optimizer = optim.Adam(params=embd_learner.parameters(), lr=lr)\n",
    "\n",
    "for epo in tqdm(range(num_epochs), desc='epoch'):\n",
    "    embd_learner.train()\n",
    "    epoch_loss = []\n",
    "    cnt = 0\n",
    "    for sent in tqdm(train_sents, desc='sent'):\n",
    "        if len(sent) < 10: continue # skip very short sentences\n",
    "        cnt += 1\n",
    "        if (cnt+1)%500 == 0: print('avg loss until step {}: {}'.format(cnt+1, np.mean(epoch_loss)))\n",
    "        for center_idx in range(0,len(sent)):\n",
    "            idx_pairs = get_mini_batch(word_to_idx, sent, center_idx, window_size)\n",
    "            \n",
    "            # Step 1: Clear the gradients \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Step 2: Compute the forward pass of the model\n",
    "            sim_scores = embd_learner([(pp[0],pp[1]) for pp in idx_pairs])\n",
    "            #print('\\nprobs', probs)\n",
    "            #print('log probs', torch.log(probs))\n",
    "\n",
    "            # Step 3: Compute the loss value that we wish to optimize\n",
    "            true_labels = torch.tensor([pair[2] for pair in idx_pairs], dtype=torch.float).to(device)\n",
    "            loss = -torch.dot(sim_scores, true_labels)\n",
    "            #loss = -torch.dot(torch.log(probs), torch.tensor([pair[2] for pair in idx_pairs], dtype=torch.float).to(device)) # cross-entropy loss\n",
    "            #loss = -torch.dot(10.*(probs-1), true_labels)\n",
    "\n",
    "            # Step 4: Propagate the loss signal backward\n",
    "            loss.backward()\n",
    "\n",
    "            # Step 5: Trigger the optimizer to perform one update\n",
    "            optimizer.step()\n",
    "            \n",
    "            #print(loss.detach().numpy())\n",
    "            epoch_loss.append(loss.cpu().detach().numpy())\n",
    "            \n",
    "        \n",
    "            \n",
    "    print('avg loss in epoch {}: {:4f}'.format(epo, np.mean(epoch_loss)))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cim/staff/uhac002/PycharmProjects/ScratchPad/venv_nlp/lib/python3.7/site-packages/ipykernel_launcher.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/home/cim/staff/uhac002/PycharmProjects/ScratchPad/venv_nlp/lib/python3.7/site-packages/ipykernel_launcher.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "# recompute all words embeddings (after training)\n",
    "word_vecs = {}\n",
    "for word in vocab:\n",
    "    vec = get_word_vec(embd_learner,word)\n",
    "    if vec is not None:\n",
    "        word_vecs[word] = vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('we', 0.9999999), ('it', 0.8605505), ('that', 0.85932636), ('the', 0.85546327), ('of', 0.85371566), (',', 0.8522324), ('in', 0.85171664), ('.', 0.8510599), ('to', 0.8467591), ('as', 0.8466495)]\n"
     ]
    }
   ],
   "source": [
    "# see whether the similar words found by the trained word embeddings make sense \n",
    "print(get_most_similar('we', word_vecs)[:10]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_nlp",
   "language": "python",
   "name": "venv_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
