{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ranked Information Retrieval\n",
    "In this exercise, we will use different strategies to vectorize text and compare text simiarity. We perform our experiments on the 'Learned' genre from the Brown corpus, because it has a decent size (around 182k tokens) and includes texts from a good diverse of sources (text source information of different Brown genres can be found [here](http://clu.uni.no/icame/manuals/BROWN/INDEX.HTM)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "all_sents = brown.sents(categories='learned')\n",
    "all_words = brown.words(categories='learned')\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "print('token num', len(all_words))\n",
    "print('sentence num', len(all_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Build vocabulary\n",
    "Extract all types to build the vocabulary list. You may consider whether to use some text normalization techniques (e.g. stop-words removal, stemming, lemmatizing) or not. If you are not sure which text normalization techniques to use, you may try different combinations and compare their performance in downstream tasks. Name the resulting vocabulary list **vocab**; we will use it in later tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(all_words, stemmer=False, stopremover=False, lemmatizer=False, lower=True):\n",
    "    # this function should return a list with each element a unique type\n",
    "    # by default no text normalization is applied\n",
    "    pass\n",
    "\n",
    "vocab = get_vocab(all_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Find similar sentences with Jaccard coefficient\n",
    "Given an arbitrary sentence in the genre, we would like to find the N sentences most similar to it. In the function below, you should use Jaccard coefficient to find the top N similar sentences. **NOTE**: if you have used certain text normalization techniques when you build the vocabulary (task 1), you should use the same ones when you compute the Jaccard coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_jscore(idx1, idx2, all_sents, vocab):\n",
    "    pass # measure similarity between all_sents[idx1] and all_sents[idx2] using Jaccard\n",
    "\n",
    "def jaccard_scores(target_sent_idx, all_sents, vocab):\n",
    "    target_sent = all_sents[target_sent_idx]\n",
    "    similarity_scores = []\n",
    "    for i in range(len(all_sents)):\n",
    "        similarity_scores.append(get_jscore(target_sent_idx,i,all_sents,vocab))\n",
    "    return similarity_scores\n",
    "\n",
    "# given the similarity scores, print the most similary N sentences\n",
    "target_idx = 123 # feel free to change this to your favourite index\n",
    "print('===Target Sentence===')\n",
    "print(' '.join(all_sents[target_idx]))\n",
    "\n",
    "jsim_scores = jaccard_scores(target_idx, all_sents, vocab)\n",
    "top_n = 10 # how many most similar sents you want \n",
    "sorted_jsim = sorted(jsim_scores,reverse=True)\n",
    "for i,ss in enumerate(sorted_jsim[:top_n]):\n",
    "    sent_idx = jsim_scores.index(ss)\n",
    "    print('\\n===Similar sentence no.{}==='.format(i+1))\n",
    "    print(' '.join(all_sents[sent_idx]))\n",
    "    print('similarity score:',ss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Find similar sentences with cosine of TF vectors \n",
    "Unlike the last task that uses Jaccard coefficients to measure sentences similarity, here we turn to cosine of Trem-Frequency (tf) vecotrs. Try both the raw-tf version and the logarithm-tf version.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cosine(vec1, vec2):\n",
    "    # given two vectors, this function should return their cosine similarity\n",
    "    pass\n",
    "\n",
    "def build_tf_vecs(all_sents, vocab):\n",
    "    # this function should return, for each sentence, a vector representation\n",
    "    pass \n",
    "\n",
    "def get_cos_scores(target_idx, all_vecs):\n",
    "    tf_scores = []\n",
    "    for i in tqdm(range(len(all_vecs)), desc='computing cosine similarities'):\n",
    "        ss = get_cosine(all_vecs[i], all_vecs[target_idx])\n",
    "        tf_scores.append(ss)\n",
    "    return tf_scores\n",
    "\n",
    "# given the similarity scores, print the most similary N sentences\n",
    "target_idx = 123 # feel free to change this to your favourite index\n",
    "print('\\n===Target Sentence===')\n",
    "print(' '.join(all_sents[target_idx]))\n",
    "\n",
    "all_vecs = build_tf_vecs(all_sents, vocab)\n",
    "tfcos_scores = get_cos_scores(target_idx, all_vecs)\n",
    "top_n = 10 # how many most similar sents you want \n",
    "sorted_tfcos = sorted(tfcos_scores,reverse=True)\n",
    "for i,ss in enumerate(sorted_tfcos[:top_n]):\n",
    "    sent_idx = tfcos_scores.index(ss)\n",
    "    print('\\n===Similar sentence no.{}==='.format(i+1))\n",
    "    print(' '.join(all_sents[sent_idx]))\n",
    "    print('similarity score:',ss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Find similar sentences with cosine of TF-IDF vectors \n",
    "Use tf-idf vectors cosine similarity to find similar sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tfidf_vecs(all_sents, vocab):\n",
    "    # this function should return, for each sentence, a tf-idf vector representation\n",
    "    pass \n",
    "\n",
    "# given the similarity scores, print the most similary N sentences\n",
    "target_idx = 123 # feel free to change this to your favourite index\n",
    "print('\\n===Target Sentence===')\n",
    "print(' '.join(all_sents[target_idx]))\n",
    "\n",
    "all_vecs = build_tfidf_vecs(all_sents, vocab)\n",
    "tfidf_cos_scores = get_cos_scores(target_idx, all_vecs)\n",
    "top_n = 10 # how many most similar sents you want \n",
    "sorted_tfidf_cos = sorted(tfidf_cos_scores,reverse=True)\n",
    "for i,ss in enumerate(sorted_tfidf_cos[:top_n]):\n",
    "    sent_idx = tfidf_cos_scores.index(ss)\n",
    "    print('\\n===Similar sentence no.{}==='.format(i+1))\n",
    "    print(' '.join(all_sents[sent_idx]))\n",
    "    print('similarity score:',ss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_nlp",
   "language": "python",
   "name": "venv_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
