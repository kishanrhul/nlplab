{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Layer Perceptron for Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>target</th>\n",
       "      <th>target_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2574</th>\n",
       "      <td>From: bdm@cs.rit.edu (Brendan D McKay)\\nSubjec...</td>\n",
       "      <td>17</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7242</th>\n",
       "      <td>Subject: Re: Who's next? Mormons and Jews?\\nFr...</td>\n",
       "      <td>19</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4437</th>\n",
       "      <td>From: msf@skaro.as.arizona.edu (Michael Fulbri...</td>\n",
       "      <td>12</td>\n",
       "      <td>sci.electronics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2242</th>\n",
       "      <td>From: hernlem@chess.ncsu.edu (Brad Hernlem)\\nS...</td>\n",
       "      <td>17</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3870</th>\n",
       "      <td>From: umturne4@ccu.umanitoba.ca (Daryl Turner)...</td>\n",
       "      <td>10</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7827</th>\n",
       "      <td>From: cdkaupan@eos.ncsu.edu (CARL DAVID KAUPAN...</td>\n",
       "      <td>10</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3438</th>\n",
       "      <td>From: mathew &lt;mathew@mantis.co.uk&gt;\\nSubject: A...</td>\n",
       "      <td>0</td>\n",
       "      <td>alt.atheism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8452</th>\n",
       "      <td>From: huub@cwi.nl (Huub Bakker)\\nSubject: wait...</td>\n",
       "      <td>5</td>\n",
       "      <td>comp.windows.x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4700</th>\n",
       "      <td>From: mkramer@world.std.com (Mark W Kramer)\\nS...</td>\n",
       "      <td>17</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6714</th>\n",
       "      <td>From: eshneken@ux4.cso.uiuc.edu (Edward A Shne...</td>\n",
       "      <td>17</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11314 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                content  target  \\\n",
       "2574  From: bdm@cs.rit.edu (Brendan D McKay)\\nSubjec...      17   \n",
       "7242  Subject: Re: Who's next? Mormons and Jews?\\nFr...      19   \n",
       "4437  From: msf@skaro.as.arizona.edu (Michael Fulbri...      12   \n",
       "2242  From: hernlem@chess.ncsu.edu (Brad Hernlem)\\nS...      17   \n",
       "3870  From: umturne4@ccu.umanitoba.ca (Daryl Turner)...      10   \n",
       "...                                                 ...     ...   \n",
       "7827  From: cdkaupan@eos.ncsu.edu (CARL DAVID KAUPAN...      10   \n",
       "3438  From: mathew <mathew@mantis.co.uk>\\nSubject: A...       0   \n",
       "8452  From: huub@cwi.nl (Huub Bakker)\\nSubject: wait...       5   \n",
       "4700  From: mkramer@world.std.com (Mark W Kramer)\\nS...      17   \n",
       "6714  From: eshneken@ux4.cso.uiuc.edu (Edward A Shne...      17   \n",
       "\n",
       "               target_names  \n",
       "2574  talk.politics.mideast  \n",
       "7242     talk.religion.misc  \n",
       "4437        sci.electronics  \n",
       "2242  talk.politics.mideast  \n",
       "3870       rec.sport.hockey  \n",
       "...                     ...  \n",
       "7827       rec.sport.hockey  \n",
       "3438            alt.atheism  \n",
       "8452         comp.windows.x  \n",
       "4700  talk.politics.mideast  \n",
       "6714  talk.politics.mideast  \n",
       "\n",
       "[11314 rows x 3 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "import pandas as pd\n",
    "df = pd.read_json('https://raw.githubusercontent.com/selva86/datasets/master/newsgroups.json')\n",
    "df.sample(frac=1) # shuffle data rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total data size: 5000, label type num: 20\n",
      "labels: ['rec.motorcycles', 'soc.religion.christian', 'comp.os.ms-windows.misc', 'comp.sys.mac.hardware', 'comp.sys.ibm.pc.hardware', 'misc.forsale', 'comp.graphics', 'rec.sport.baseball', 'talk.politics.misc', 'sci.electronics', 'sci.med', 'sci.crypt', 'sci.space', 'comp.windows.x', 'talk.religion.misc', 'talk.politics.mideast', 'rec.sport.hockey', 'alt.atheism', 'talk.politics.guns', 'rec.autos']\n"
     ]
    }
   ],
   "source": [
    "wanted_doc_num = 5000 #the dataset is quite large; hence for illustration purpose I only use a small par of it; feel free to use more data points\n",
    "raw_labels = df.target_names.values.tolist()[:wanted_doc_num]\n",
    "docs = df.content.values.tolist()[:wanted_doc_num]\n",
    "\n",
    "assert len(docs) == len(raw_labels)\n",
    "label_list = list(set(raw_labels))\n",
    "labels = [label_list.index(rl) for rl in raw_labels] # transfer raw labels (strings) to integer numbers\n",
    "print('total data size: {}, label type num: {}'.format(len(docs), len(label_list)))\n",
    "print('labels:', label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: abarden@tybse1.uucp (Ann Marie Barden)\n",
      "Subject: X-Terminal Config. file question\n",
      "Organization: Tybrin Corporation, Shalimar, FL\n",
      "Distribution: usa\n",
      "Lines: 19\n",
      "\n",
      "  QUESTION:\n",
      "  What is the EXACT entry (parameter and syntax please), in the X-Terminal\n",
      "configuration file (loaded when the X-Terminal boots), to add another system \n",
      "to the TCP/IP access control list?   \n",
      "\n",
      "  BACKGROUND:\n",
      "  I have two unix systems, 1. an AT&T 3B2 running X11R3 and MIT's X11R4 and \n",
      "2. a Sun SS10 without any X.  \n",
      "  I want to have a window to the Sun and the 3B2 on the NCD X-Terminal at the\n",
      "same time.  I can do this if I manually set the Network Parameter TCP/IP\n",
      "Access Control List to off, then login to my telnet session. Not Great!  \n",
      "  I've tried to get \"xhost\" to work and failed.  Either my syntax is wrong\n",
      "or the X11R3 implementation is bogus.  \n",
      "  I am trying to edit the NCD configuration file that is loaded when the \n",
      "NCD boots.  No matter what entry I add or edit, the NCD still boots with\n",
      "the TCP/IP Access Control list containing only the 3B2.\n",
      "  My manuals are worthless so any help would be most appreciated!!  Thanks!\n",
      "\n",
      "Ann Marie Barden  \tabarden@afseo.eglin.af.mil\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# take a look at some documents in the dataset\n",
    "print(docs[19])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size 3000, dev size 1000, test size 1000\n"
     ]
    }
   ],
   "source": [
    "train_ratio, dev_ratio, test_ratio = 0.6, 0.2, 0.2\n",
    "train_docs = docs[:int(len(docs)*train_ratio)]\n",
    "train_labels = labels[:int(len(docs)*train_ratio)]\n",
    "\n",
    "dev_docs = docs[int(len(docs)*train_ratio):int(len(docs)*(train_ratio+dev_ratio))]\n",
    "dev_labels = labels[int(len(docs)*train_ratio):int(len(docs)*(train_ratio+dev_ratio))]\n",
    "\n",
    "test_docs = docs[-int(len(docs)*(test_ratio)):]\n",
    "test_labels = labels[-int(len(docs)*(test_ratio)):]\n",
    "\n",
    "print('train size {}, dev size {}, test size {}'.format(len(train_labels), len(dev_labels), len(test_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train vec size (3000, 2000)\n",
      "dev vec size (1000, 2000)\n"
     ]
    }
   ],
   "source": [
    "# create vector representations; \n",
    "# TODO: consider to apply necessary text cleaning/normalization techniques, e.g. remove all emails, remove the section headers (QUESTION, BACKGROUND) and meta-information (From, Lines)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vec_dim = 2000 # feel free to use longer vecs\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=vec_dim)\n",
    "train_vecs = tfidf_vectorizer.fit_transform(train_docs)\n",
    "dev_vecs = tfidf_vectorizer.transform(dev_docs)\n",
    "\n",
    "print('train vec size', train_vecs.shape)\n",
    "print('dev vec size', dev_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the neural model, an MLP\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\" A perceptron has only two linear layers and an additional softmax layer on top\"\"\"\n",
    "    def __init__(self, input_dim, out_dim, dp_rate):\n",
    "        super(MLP, self).__init__()\n",
    "        self.hidden_layer = nn.Linear(input_dim, input_dim) # we let the hidden layer width be the same as the input layer\n",
    "        # nn.Linear initialize weight using Glorot initialization, see https://discuss.pytorch.org/t/how-are-layer-weights-and-biases-initialized-by-default/13073 \n",
    "        self.output_layer = nn.Linear(input_dim, out_dim)\n",
    "        # self.softmax = nn.Softmax(dim=1) # if you use nn.CrossEntropyLoss as loss, it includes softmax computation and so you don't need softmax layer in your net\n",
    "        self.dropout = nn.Dropout(dp_rate)\n",
    "       \n",
    "    def forward(self, x_in, act_fnc):\n",
    "        z1 = self.dropout(x_in) # output of the input layer, after dropout\n",
    "        z2 = act_fnc(self.hidden_layer(z1)) # output of the hidden layer\n",
    "        logits = self.output_layer(z2)\n",
    "        # probs = self.softmax(logits) # if you use nn.CrossEntropyLoss as loss, it includes softmax computation and so you don't need softmax layer in your net\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model\n",
    "relu = torch.nn.ReLU() # we use ReLU as activation function; feel free to try others\n",
    "dropout_rate = 0.5 # dropout rate\n",
    "model = MLP(vec_dim,len(label_list),dropout_rate) # since this is a classification problem, the output dimension of the MLP should be the number of classes\n",
    "loss_fnc = torch.nn.CrossEntropyLoss() # cross entropy loss\n",
    "\n",
    "# hyper parameters\n",
    "n_epochs = 30 # number of epoch (i.e. number of iterations)\n",
    "batch_size = 64 # usually set to the powers of 2, e.g. 2,4,8,32,64,128. \n",
    "lr = 0.001 # initial learning rate\n",
    "\n",
    "# init optimizer and scheduler (lr adjustor)\n",
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=lr) # use Adam as the optimizer\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1) # decays the learning rate of each parameter group by gamma every step_size epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 0 the accuracy on dev set is 0.542\n",
      "learning rate 0.001\n",
      "best model updated; new best acc 0.542\n",
      "\n",
      "---> after epoch 1 the accuracy on dev set is 0.702\n",
      "learning rate 0.001\n",
      "best model updated; new best acc 0.702\n",
      "\n",
      "---> after epoch 2 the accuracy on dev set is 0.737\n",
      "learning rate 0.001\n",
      "best model updated; new best acc 0.737\n",
      "\n",
      "---> after epoch 3 the accuracy on dev set is 0.759\n",
      "learning rate 0.001\n",
      "best model updated; new best acc 0.759\n",
      "\n",
      "---> after epoch 4 the accuracy on dev set is 0.764\n",
      "learning rate 0.001\n",
      "best model updated; new best acc 0.764\n",
      "\n",
      "---> after epoch 5 the accuracy on dev set is 0.764\n",
      "learning rate 0.001\n",
      "\n",
      "---> after epoch 6 the accuracy on dev set is 0.756\n",
      "learning rate 0.001\n",
      "\n",
      "---> after epoch 7 the accuracy on dev set is 0.753\n",
      "learning rate 0.001\n",
      "\n",
      "---> after epoch 8 the accuracy on dev set is 0.755\n",
      "learning rate 0.001\n",
      "\n",
      "---> after epoch 9 the accuracy on dev set is 0.764\n",
      "learning rate 0.001\n",
      "\n",
      "---> after epoch 10 the accuracy on dev set is 0.772\n",
      "learning rate 0.0001\n",
      "best model updated; new best acc 0.772\n",
      "\n",
      "---> after epoch 11 the accuracy on dev set is 0.773\n",
      "learning rate 0.0001\n",
      "best model updated; new best acc 0.773\n",
      "\n",
      "---> after epoch 12 the accuracy on dev set is 0.776\n",
      "learning rate 0.0001\n",
      "best model updated; new best acc 0.776\n",
      "\n",
      "---> after epoch 13 the accuracy on dev set is 0.776\n",
      "learning rate 0.0001\n",
      "\n",
      "---> after epoch 14 the accuracy on dev set is 0.78\n",
      "learning rate 0.0001\n",
      "best model updated; new best acc 0.78\n",
      "\n",
      "---> after epoch 15 the accuracy on dev set is 0.781\n",
      "learning rate 0.0001\n",
      "best model updated; new best acc 0.781\n",
      "\n",
      "---> after epoch 16 the accuracy on dev set is 0.782\n",
      "learning rate 0.0001\n",
      "best model updated; new best acc 0.782\n",
      "\n",
      "---> after epoch 17 the accuracy on dev set is 0.778\n",
      "learning rate 0.0001\n",
      "\n",
      "---> after epoch 18 the accuracy on dev set is 0.775\n",
      "learning rate 0.0001\n",
      "\n",
      "---> after epoch 19 the accuracy on dev set is 0.776\n",
      "learning rate 0.0001\n",
      "\n",
      "---> after epoch 20 the accuracy on dev set is 0.776\n",
      "learning rate 1.0000000000000003e-05\n",
      "\n",
      "---> after epoch 21 the accuracy on dev set is 0.774\n",
      "learning rate 1.0000000000000003e-05\n",
      "\n",
      "---> after epoch 22 the accuracy on dev set is 0.777\n",
      "learning rate 1.0000000000000003e-05\n",
      "\n",
      "---> after epoch 23 the accuracy on dev set is 0.777\n",
      "learning rate 1.0000000000000003e-05\n",
      "\n",
      "---> after epoch 24 the accuracy on dev set is 0.778\n",
      "learning rate 1.0000000000000003e-05\n",
      "\n",
      "---> after epoch 25 the accuracy on dev set is 0.777\n",
      "learning rate 1.0000000000000003e-05\n",
      "\n",
      "---> after epoch 26 the accuracy on dev set is 0.776\n",
      "learning rate 1.0000000000000003e-05\n",
      "\n",
      "---> after epoch 27 the accuracy on dev set is 0.777\n",
      "learning rate 1.0000000000000003e-05\n",
      "\n",
      "---> after epoch 28 the accuracy on dev set is 0.776\n",
      "learning rate 1.0000000000000003e-05\n",
      "\n",
      "---> after epoch 29 the accuracy on dev set is 0.777\n",
      "learning rate 1.0000000000000003e-05\n"
     ]
    }
   ],
   "source": [
    "best_acc = -1.\n",
    "best_model = None\n",
    "import copy\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for epoch_i in range(n_epochs):\n",
    "    # the inner loop is over the batches in the dataset\n",
    "    model.train() # let pytorch know that gradients should be computed, so as to update the model\n",
    "    for idx in range(0,train_vecs.shape[0],batch_size):\n",
    "        # Step 0: Get the data\n",
    "        x_data = torch.tensor(train_vecs[idx:idx+batch_size].todense(), dtype=torch.float)\n",
    "        if x_data.shape[0] == 0: continue\n",
    "        y_target = torch.tensor(train_labels[idx:idx+batch_size], dtype=torch.int64)\n",
    "\n",
    "        # Step 1: Clear the gradients \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Step 2: Compute the forward pass of the model\n",
    "        y_pred = model(x_data, relu)\n",
    "\n",
    "        # Step 3: Compute the loss value that we wish to optimize\n",
    "        loss = loss_fnc(y_pred, y_target)\n",
    "\n",
    "        # Step 4: Propagate the loss signal backward\n",
    "        loss.backward()\n",
    "\n",
    "        # Step 5: Trigger the optimizer to perform one update\n",
    "        optimizer.step()\n",
    "    \n",
    "    # after each epoch, we can test the model's performance on the dev set\n",
    "    with torch.no_grad(): # let pytorch know that no gradient should be computed\n",
    "        model.eval() # let the model know that it in test mode, i.e. no gradient and no dropout\n",
    "        dev_data = torch.tensor(dev_vecs.todense(), dtype=torch.float)\n",
    "        dev_target = torch.tensor(dev_labels, dtype=torch.int64)\n",
    "        dev_prediction = model(dev_data, relu)\n",
    "        pred_labels = [np.argmax(dp.numpy()) for dp in dev_prediction]\n",
    "        acc = accuracy_score(dev_target, pred_labels)\n",
    "        print('\\n---> after epoch {} the accuracy on dev set is {}'.format(epoch_i, acc))\n",
    "        for param_group in optimizer.param_groups:\n",
    "            print('learning rate', param_group['lr'])\n",
    "        \n",
    "        # save the best model\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_model = copy.deepcopy(model.state_dict())\n",
    "            print('best model updated; new best acc',acc)\n",
    "            \n",
    "    # (optional) adjust learning rate according to the scheduler\n",
    "    scheduler.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc on test data 0.795\n"
     ]
    }
   ],
   "source": [
    "# test on the test set\n",
    "\n",
    "# load the best model weights\n",
    "model.load_state_dict(best_model) \n",
    "\n",
    "with torch.no_grad(): \n",
    "    model.eval()\n",
    "    test_vecs = tfidf_vectorizer.transform(test_docs)\n",
    "    test_data = torch.tensor(test_vecs.todense(), dtype=torch.float)\n",
    "    test_target = torch.tensor(test_labels, dtype=torch.int64)\n",
    "    test_prediction = model(test_data, relu)\n",
    "    pred_labels = [np.argmax(dp.numpy()) for dp in test_prediction]\n",
    "    acc = accuracy_score(test_target, pred_labels)\n",
    "    print('acc on test data', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO:\n",
    "* Print the dimension of each layer inside MLP to make sure you understand the how it works (e.g. print(z1.shape)). \n",
    "* Try different neural architectures (e.g. different dimension of the hidden layer, different activation functions, different number of hidden layers, different dropout rate) and see their influence on the performance\n",
    "* Try different hyper parameters (e.g. num of epoch, learning rate, etc.) and see their influence on the performance\n",
    "* Try to implement *early stoping* by following the instructions [here](https://github.com/pytorch/ignite/issues/560)\n",
    "* Try different weight initialization strategies by following the top answer [here](https://stackoverflow.com/questions/49433936/how-to-initialize-weights-in-pytorch)\n",
    "* Try different length of vector dimension and see its influence on the performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "* Pytorch basics tutorial (**highly recommended if you have no deep learning development experiences before**): https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html \n",
    "* learning rate scheduler in pytorch: https://pytorch.org/docs/stable/_modules/torch/optim/lr_scheduler.html \n",
    "* loss functions in pytorch: https://pytorch.org/docs/stable/nn.html#loss-functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_nlp",
   "language": "python",
   "name": "venv_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
